
Interestingly the best timing is achieved with 4096 rays

very nice gain:  560 -> 600ish  by rearranging the list counts so they loop could get vectorized

ID scattering succccckkks...

Probably lots of gains to be had from switching to 8-ray packets to avoid the gathers in the traversal loops

  Yep.  611Krays to 820Krays for a threshold of 32.  Woot! 

  Dumb packet tracing is 793.  

More perf to be had by micro-optimizing the packet case.  This one is a sizable chunk of our render time
  
There is probably even more to be gained by adding another customization for the coherent top case.
  The first few nodes can use a non-gather path

  Boosted to 850 from 820 by tweaking Sah cost


Tooled around with the packet-triangle test.  _mm256_set1_ps(x) does not do what I thoguht it would in MSVC.
They like to generate a shuffle + vinsert.  Lame...
  To do the right thing you need _mm256_broadcastss_ps( _mm_set_ss(x) )


Considering a few alternatives for the fast path at the top.
   1.  "coherent range stack"
       Test single node, generate mask array, find runs of SIMD groups with all rays in agreement (coherent)
         Store all the runs
           
            CCCCCC IIII CCCCC IIIII
        Build a list of ray ranges from the stream to test against child node.   Pull out incoherent groups into ID path as we go.

  2.  "Utilization check"
      Test single node, generate mask array, shunt SIMD groups down the indexed path when utilization drops too low
          
           

Tried rearranging the ray packet so that we don't gather T, U,V, and friends, but instead scatter them on intersect test.
   Performance tanked.   Rather surprising.   I'm not sure what's to blame.  Maybe cache thrash?

   Got a gain (855 -> 870ish) by removing the gather of u,v,t, and friends when assembling packets.
     Instead, we use a cache field in teh packet and write back the things that got written by an isect test

GOt new sexy partitioning/reordering algorithm implemented.
  Down to 650 from 850.  Sad.   New path is a lot more elaborate and complex.
    Lots more tuning to be done, but still, I was expecting better...   I guess having two copies of the ray data is being too mean to 
      the cache...

Jump from 650 to 744 just be recomputing the direction recips during reordering.
  That aughtta tell us something.   Tells us that we're abusing cache.

  Pulling dumping packets to stack instead of pulling from stream makes it faster still

  Version using one scratch array and multiple loops was slower (730ish).
    Trying to make 'WritebackPacket' use dword streaming was a failed experiment.  For some reason movntdi sucks here.

    Discovered that MS doesn't use vmovaps unless you use the ymm intrinsic.  Writing to an __m256 pointer doesn't do it

Got to 780 by adjusting split threshold.  I was using 4*nGroups, should have been 4*incoherent. 
  Means that there's a 50% SIMD utilization improvement

 Massive perf boost by increasing threshold for single-packet traversal.
   950!   For threshold of <= 256 groups, which is half the stream size.

This result occurs because 'binraysbyoctant' does all the work for us.
   We don't hit the top traversal loop at all.   Just doing dumb packet traversal for octant-sorted packets
  is a very effective strategy.

  This makes total sense now.    I'm binning by octant which means I've only got 64 groups in flight.
    So the streaming ray tracing is by nature less effective.  We don't amortize as much.


Tried regular partition trav.  No help.  Reordering is useful, but not doing anything smart is even better.
  
  I'm wondering how much of a difference the double-node intersection makes.
    Possible that less stack manipulation is important
  

 Oddly, reversing my traversal order has caused equivalet perf for partition-style traversal.
   bizarre.   Something is wrong with my ordered traversal logic?  Or maybe true intersection distance is just better overall...

  
  Eureka.  Reversed octants breaks 1000 for the reorder path.
    But why?!?!?!?

 Because I was always passing octant freaking 8!!!!!!!  gaaah!
   This is what happens when I code when I'm tired

   wooot!  With sane ordered traversal, now hitting 1200!   Using new sexy octant-aware packet path (1195 without it)
    Plain partition trace is 1100

Intel bit manipulation instructions are awesome!!!  Truly awesome.  So many cool bit-tricks you can do with them.
 count leading zeros, extract bits into a contiguous set (morton decode), set lowest/highest bit to zero.
 Truly some awesomeness there.... Worth a blog post in itself

 Inlining the 'packettest' was a huge gain (rather unsurprising in retrospec).
   With that, and some bit manipulation tweaks, and early-out for complete miss, up to 1374 now!

Rearrange stack, and remoe branches from AABB intersect loop:  1408 :)
  hmmm, 1408 was with a bug where I never treated packs as 'hit'
    
But fixing that makes it drop.   This means that reordering overhead maybe not so bad...??

Holy crap.  1447 if I drop the additional group arrays and just always reorder all the pure hit packs with the missed ones.
  Removing all hte excess stack arrays was very helpful.

Seperating out coherent hit packs does not seem to be worth it.   Maybe these just don't exist??

1447 -> 1530 by 2x unrolling the group/aabb isect loop

according to agner, only FP muls can dual-issue.  add,sub,min,max all have one cycle throughput.
  lame.  Madd, however, can also dual-issue.  Worth looking at


  wooot!!!  1660 with pandora tax, by removing the gathers during reordering and replacing with "half-clean" and masked moves.

  Now I'm beating the embree streamtrace stuffs.    

  Still faster to redo the 1/D division

CodeXL tells me that I spend a bunch of time waiting on those divs, 
 and a bunch more time with loads and stores to stack.  I still wonder whether fetching the ray data is a bottleneck




 
        // TODO: Can generate 'hits' by broadcasting mask, and with a masks vector (1<<0,1<<1,1<<2, ... )
        //  and then compare with it.

        // to generate permute is harder...
        //   possibility: compact hits and misses, reverse the latter, and or together
        //       do a pre-fix sum and an anti-prefix sum
        //
        //    After a compaction, the index of the OLD element at compacted location i is:
        //      prefix(i) + anti-prefix(i).  Maybe can permute   an identity vector this way...
        //      garbage at top can be ignored
        //  NO:  THIS PREFIX SUM THING WILL NOT WORK.  ITS NOT ANTI-PREFIX(I) its ANTI-PREFIX[idx[i]].  its circular.
        //
        //   To do prefix sum:
        //     ......
        //   0  0   0  0    0  0  0  0
        //      V0     V2      V4    V6
        //          Q0 Q0         Q2  Q2
        //                  h0 h0 h0 h0
        //  Where:
        //    V[i] is ith element
        //    Q0 = V0 + V1
        //    Q2 = V4 + V5
        //    H0 = V0 + V1 + V2 + V3 = Q0 + Q1


Going to try a new approach to ray reordering
   Ray data: 0x,oy,oz,dx,dy,dz,t,id.   Fits exactly in 8 lanes
 
 I could fetch 8 rays at a time and do an 8x8 transpose


  
 OH FOO!  i HAVE HAD A BUG FOR MONTHS, SINCE I FIRST LEFT IT.   BACK TO THE DRAWING BOARD...

 Whew, just a stupid mistake.

 GOOD NEWS!   Nice boost from reorganizing the packets and doing a transpose instead of all those mask-stores.

 Now I am hitting 1770 m/rays

 
 Got a nice boost up to 1800 by converging all the tzcnt/bsl loops to use uint64 instead of ujint.
 Partial reg stalls matter it would seem...

 another minor boost by twiddling cache layout of the ray packet.
  
Next idea is to not do popcnt in the intersect loop but just store the masks  and move on

Curious, it is must faster to do the miss case first in ReorderRays.  Wonder why???

Took a look at pleucker and other exotic aabb tests, but the lack of an intersection distance makes early termination problematic.
 
 Nice gain by observing that  this:
        __m256 l0   = _mm256_min_ps( tmax0, pack0.TMax );
        __m256 l1   = _mm256_min_ps( tmax1, pack1.TMax );
        __m256 hit0 = _mm256_andnot_ps( tmax0, _mm256_cmp_ps(tmin0,l0,_CMP_LE_OQ) );
        __m256 hit1 = _mm256_andnot_ps( tmax1, _mm256_cmp_ps(tmin1,l1,_CMP_LE_OQ) );
     
 Has better  dependency structure than this:   
        tmax0  = _mm256_min_ps( tmax0, pack0.TMax );
        tmax1   = _mm256_min_ps( tmax1, pack1.TMax );
        __m256 hit0 = _mm256_andnot_ps( tmax0, _mm256_cmp_ps(tmin0,tmax0,_CMP_LE_OQ) );
        __m256 hit1 = _mm256_andnot_ps( tmax1, _mm256_cmp_ps(tmin1,tmax1,_CMP_LE_OQ) );

The and no longer depends on the mins.   We're about about 1830 now
  WHAT ARE YOU TALKING ABOUT?? THEY DO DEPEND.  DIFFERENCE IS THAT COMPILER'S USING REGS DIFFERENT


I got it up to 1920 by writing ReadRays in assembly.  then I tried to remove the divides, only hitting 1750 now
That seems wrong...   Seems that replacing 3 divides by a crapton of unpacks and extra loads is not a net gain

Not re-reading dirs until the isect test got it up to ~1840, but removing divs still slow.
Dont understand...   Well, magnitude of the loss equals the ocst of the extra readdirs() step before intersection.
A whole bunch of shuffles doens't compensate for the divs

Switching divs to newton-raphson took it from 1920 ->1940

tried a jump table for that read-rays nonsense.  crazy idea.  Never try that again.

Tried unrolling loops too, but it seems indifferent

Observed nice gains in ReorderRays by using SIMD prefix sum (see above) to derive the addresses, then doing
  a bunch of extracts and stores.  Noticing that L1 cache misses on the ray IDs and such are
  starting to make a big difference.  The prefetches are important for that last little bit of perf

Prefetching triangle verts in packettrace_octant.  Hitting 1980-1990 now

WOOT  2007 with rearrangement of BVH to store index ptr directly in BVH...
2016 with T1 instead of T0 node prefetch for PacketTrace_Octant.

interleaving movs is fine for co-issue.  Remember that HSW feeds 4 ops at a time to RS


Rewrote the readrays pass to use a big fat asm loop and now I'm hitting around 2030.


architectural regs suck.  I KNOW that the regs are renamed.  I aught not to have to give them arbitrary names
  just for the heck of it

 Hitting 2060 now after optimizing RemoveMissedGroups, and skipping removal/reorder
  on a full hit

  Removed divps from tri intersection:  -> ~2080

  Removed writebacks entirely and just wrote directly on hit.  Now ~2090 at times.  The way is open to some other stuff:
    1.  Replace short ID array with packet ptrs to enable easier L1 prefetch.
    2.  Prefetch that.
    3.  Seperate out the D[] from the packet, which will make packet size a power of 2



Up to 2160 by using packet ptrs instead of short groupIDs.  And that is WITHOUT the asm readrays loop.  WOOT!
With the asm --> 2170

Was 2180 after a prefetch tweak to PacketTrace_Octant.
Got up to 2190 by removing the slow loads from ReadRaysLoop_ASM
  Loads with more than two operands are SLLLOOOOOW
  
peak: 2203

Triangle preproc across the board.  Use preproc tris instead of index lists for leaf payloads.
~2280

Switch to a loop over tris INSIDE the intersector fn.  jumped 2280->2380.  wooooot!

Re-wrote ReadRaysLoop as fully inlined and now compiler is beating me.. :(

Writing a single readrays call in asm was better because I spilled less, but writing a loop, the spills
  don't matter and the compiler's superior scheduling wins out

But...  Using the 'ReadDirs' trick now gives me a payoff IF I also move the rcps() out.
  Got a nice gain 2400 ->2418 by changing:

        __m256 t0 = _mm256_unpacklo_ps(l0,l1); // 00 11 00 11
       __m256 t1 = _mm256_unpacklo_ps(l2,l3); // 00 11 00 11
       __m256 t2 = _mm256_unpackhi_ps(l0,l1); // 22 33 22 33
       __m256 t3 = _mm256_unpackhi_ps(l2,l3); // 22 33 22 33
       __m256 t4 = _mm256_unpacklo_ps(l4,l5); // 44 55 44 55
       __m256 t5 = _mm256_unpacklo_ps(l6,l7); // 44 55 44 55       
       __m256 t6 = _mm256_unpackhi_ps(l4,l5); // 66 77 66 77 
       __m256 t7 = _mm256_unpackhi_ps(l6,l7); // 66 77 66 77
        __m256 Ox    = _mm256_unpacklo_ps(t0,t1); // 00 00 00 00
        __m256 Oy    = _mm256_unpackhi_ps(t0,t1); // 11 11 11 11
        __m256 Oz    = _mm256_unpacklo_ps(t2,t3); // 22 22 22 22
        __m256 TMax  = _mm256_unpackhi_ps(t2,t3); // 33 33 33 33
        __m256 Dx    = _mm256_unpacklo_ps(t4,t5); 
        __m256 Dy    = _mm256_unpackhi_ps(t4,t5); 
        __m256 Dz    = _mm256_unpacklo_ps(t6,t7);
        __m256 RID   = _mm256_unpackhi_ps(t6,t7);
   
        RayPacket* __restrict pPacket = l.pPackets[i];
       _mm256_store_ps( (float*)&pPacket->Ox, Ox );
       _mm256_store_ps( (float*)&pPacket->DInvx, RCPNR(Dx) );
       _mm256_store_ps( (float*)&pPacket->Oy, Oy );    
       _mm256_store_ps( (float*)&pPacket->DInvy, RCPNR(Dy) );
       _mm256_store_ps( (float*)&pPacket->Oz, Oz );    
       _mm256_store_ps( (float*)&pPacket->DInvz, RCPNR(Dz) );
       _mm256_store_ps( (float*)&pPacket->TMax, TMax );
       _mm256_store_ps( (float*)pPacket->RayOffsets, RID );

 TO:

       __m256 t0 = _mm256_unpacklo_ps(l0,l1); // 00 11 00 11
       __m256 t1 = _mm256_unpacklo_ps(l2,l3); // 00 11 00 11
       __m256 t2 = _mm256_unpackhi_ps(l0,l1); // 22 33 22 33
       __m256 t3 = _mm256_unpackhi_ps(l2,l3); // 22 33 22 33
       __m256 t4 = _mm256_unpacklo_ps(l4,l5); // 44 55 44 55
       __m256 t5 = _mm256_unpacklo_ps(l6,l7); // 44 55 44 55       
       __m256 t6 = _mm256_unpackhi_ps(l4,l5); // 66 77 66 77 
       __m256 t7 = _mm256_unpackhi_ps(l6,l7); // 66 77 66 77
       t4 = RCPNR(t4);
       t5 = RCPNR(t5);
        __m256 Ox    = _mm256_unpacklo_ps(t0,t1); // 00 00 00 00
        __m256 Oy    = _mm256_unpackhi_ps(t0,t1); // 11 11 11 11
        __m256 Oz    = _mm256_unpacklo_ps(t2,t3); // 22 22 22 22
        __m256 TMax  = _mm256_unpackhi_ps(t2,t3); // 33 33 33 33
        __m256 Dx    = _mm256_unpacklo_ps(t4,t5); 
        __m256 Dy    = _mm256_unpackhi_ps(t4,t5); 
        __m256 Dz    = _mm256_unpacklo_ps(t6,t7);
        __m256 RID   = _mm256_unpackhi_ps(t6,t7);
   
        RayPacket* __restrict pPacket = l.pPackets[i];
       _mm256_store_ps( (float*)&pPacket->Ox, Ox );
       _mm256_store_ps( (float*)&pPacket->DInvx, (Dx) );
       _mm256_store_ps( (float*)&pPacket->Oy, Oy );    
       _mm256_store_ps( (float*)&pPacket->DInvy, (Dy) );
       _mm256_store_ps( (float*)&pPacket->Oz, Oz );    
       _mm256_store_ps( (float*)&pPacket->DInvz, RCPNR(Dz) );
       _mm256_store_ps( (float*)&pPacket->TMax, TMax );
       _mm256_store_ps( (float*)pPacket->RayOffsets, RID );

discovered a lot in the effort to port library into embree.
First, discovered that the "random rays in space" test is bunk.  It is not representative of real rendering workloads bc these always
  have rays that start and end near geometry and so intersection perf is underrepresented.

In embree, I'm lagging behind the raystream stuff for complex scenes.  Hypothesis is that it's hitting the packet trace path, and that
  this simply sucks for huge meshes even with reordering bc we eventually reach leaves and run out of steam.  

Possible solutions include aiming for big leaves and building packet-specific frusta, a la reshetov vertex culling.   Or, maybe, just do a tight single-ray SIMD path.

side note, discovered that builder breaks if you have a 0 scale on two axes.


So, options for future work:
 1.  Attempt to use shallower BVHs and vertex culling to avoid losing coherence
        - also amortizes cache better perhaps...
 2.  Write a tight single-ray traversal and switch to it when packet coherence drops
 3.  Attempt to reorganize BVH build?  Use treelets near leaves to improve cache?

Tried rearranging BVH into treelets.  No change.  Tried to rearrange and abuse the loops to use lower-latency instructions
 (perms instead of lots of broadcasts).  No change.  MSVC is ruining it.

I've noticed MSVC really likes to spill if you nest intrinsics inside of one another
  
